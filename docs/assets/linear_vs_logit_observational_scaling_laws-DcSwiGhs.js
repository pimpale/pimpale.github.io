import"./modulepreload-polyfill-B5Qt9EMX.js";import{c as a,j as e,R as s}from"./client-MRn4bnEa.js";import{S as i}from"./Section-DqptXD1-.js";import{A as o}from"./ArticleLayout-DLEwcD7J.js";import"./bootstrap-RBoEklL2.js";import"./Layout-DakrT0PA.js";const n=()=>e.jsx(o,{children:({Citation:t,CitationBank:r})=>e.jsxs(e.Fragment,{children:[e.jsxs(i,{id:"overview",name:"Overview",children:[e.jsx("p",{children:"Observational Scaling Laws's uses PCA over the table of benchmark scores to find the vectors that explain as much of the variance of the benchmark scores as possible."}),e.jsxs("p",{children:["However, this approach has a limitation: it doesn't work when some benchmarks are saturated, while others maintain predictive power. Note that saturation can be either at the ceiling, from the model being too good, or at the floor, from the model being too bad. Scenarios like this can occur when you test models that have a wide range of capabilities on both easy benchmarks (like SQUAD) and difficult benchmarks (like SWE-Bench Verified",e.jsx(t,{source:"https://openai.com/index/introducing-swe-bench-verified/"}),") at the same time."]}),e.jsx("p",{children:"While writing a paper, I thought of two potential fixes for this problem:"}),e.jsxs("ol",{children:[e.jsx("li",{children:"Measuring the model capability in logit space"}),e.jsx("li",{children:"Normalizing the benchmarks before doing PCA"})]}),e.jsx("p",{children:"These are probably not original ideas, but it still seems valuable to try them and publicly report whether they work."}),e.jsx("p",{children:"I first check that these approaches improve performance on a synthetic dataset (they do) and then check that they improve performance on a real world dataset (unclear)."}),e.jsx("h4",{children:"TLDR"}),e.jsx("p",{children:"TODO"})]}),e.jsxs(i,{id:"introduction",name:"Introduction",children:[e.jsxs("p",{children:["Observational Scaling Laws and the Predictability of Language Model Performance",e.jsx(t,{source:"https://arxiv.org/abs/2405.10938"}),' (henceforth abbreviated as OSL) is a paper that attempts to condense a set of benchmarks into a few numbers that are very predictive of its score on the greater set of benchmarks. For example, these numbers might represent something like "general intelligence", "coding ability", and "math ability" respectively. So, a model with low general intelligence might still do well on coding and math benchmarks, if it had a high coding and math ability.']}),e.jsx("p",{children:"However, there are diminishing returns to adding more numbers to describe the performance of the model. In this work, we focus on just the first number, which is the most predictive of the benchmark scores. OSL finds that just the first number predicts 80% of the variance in the benchmark scores."}),e.jsxs("p",{children:["This is useful because it allows us to quantify model capabilities in a more accurate way. Prior to this, many people (Owen",e.jsx(t,{source:"https://arxiv.org/abs/2401.04757"}),", Finnveden",e.jsx(t,{source:"https://www.alignmentforum.org/posts/k2SNji3jXaLGhBeYP/extrapolating-gpt-n-performance"}),") used pretraining log-FLOP as a rough proxy of capability. This is somewhat well justified, since the Chinchilla scaling laws (Hoffman et al.)",e.jsx(t,{source:"https://arxiv.org/abs/2203.15556"})," describe what the loss should be after a given pre-training budget. However, FLOP count is not the only thing that matters. Different model families have different levels of efficiency in converting FLOP to performance (i.e, different constants in the Hoffman loss equation). Additionally, how a model is post-trained also makes a huge difference when it comes to performance on benchmarks."]}),e.jsx("p",{children:"On the other hand, using OSL's"})]}),e.jsx(i,{id:"references",name:"References",children:e.jsx(r,{})})]})}),h=a.createRoot(document.getElementById("root"));h.render(e.jsx(s.StrictMode,{children:e.jsx(n,{})}));
